{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993b7358",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "993b7358",
        "outputId": "7d32a295-66f2-461d-df82-c6ea4fe1fddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DXJXZFgd2bUE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXJXZFgd2bUE",
        "outputId": "d36305ba-d4ca-4e2e-a124-282479c88321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/event_extraction\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/event_extraction/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8f106af",
      "metadata": {
        "id": "c8f106af"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from src.event_argument_dataset import EventArgumentDataset\n",
        "from src.eventtype_retriever import EventTypeRetriever\n",
        "from src.utils.data_utils import build_labels, load_json_or_jsonl\n",
        "from src.utils.device_util import getDeviceInfo\n",
        "from src.utils.compute_metric import compute_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d811668",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d811668",
        "outputId": "b9a2775d-8b44-4978-fb53-bbc4f17c5d21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded labels from /content/drive/MyDrive/event_extraction/processing_data/event_types.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ---------------------- Config ----------------------\n",
        "DEVICE = getDeviceInfo()\n",
        "BART_MODEL = \"facebook/bart-base\"\n",
        "MAX_LENGTH = 256\n",
        "OUTPUT_MAX_LENGTH = 64\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "LR = 1e-5\n",
        "TOP_K = 1\n",
        "# CONTEXT_PATH = \"\"\n",
        "CONTEXT_PATH = '/content/drive/MyDrive/event_extraction/'\n",
        "CHECKPOINT_DIR = f\"{CONTEXT_PATH}checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "TRAIN_JSON_PATH = f\"{CONTEXT_PATH}processing_data/train.json\"\n",
        "VAL_JSON_PATH = f\"{CONTEXT_PATH}processing_data/dev.json\"\n",
        "TEST_JSON_PATH = f\"{CONTEXT_PATH}processing_data/test.json\"\n",
        "ONTOLOGY_PATH = f\"{CONTEXT_PATH}ontoloy/event_role_WIKI_q.json\"\n",
        "LABEL_CACHE_PATH = f\"{CONTEXT_PATH}processing_data/event_types.json\"\n",
        "\n",
        "event_types = build_labels(TRAIN_JSON_PATH, LABEL_CACHE_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b57614f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b57614f6",
        "outputId": "d72f0f9b-e5f4-4c93-88e0-05d774a9ed5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartScaledWordEmbedding(50266, 768, padding_idx=1)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# ---------------------- Load tokenizer & model ----------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(BART_MODEL)\n",
        "special_tokens = [\"<tgr>\"]\n",
        "tokenizer.add_tokens(special_tokens)\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained(BART_MODEL).to(DEVICE)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82674f8f",
      "metadata": {
        "id": "82674f8f"
      },
      "outputs": [],
      "source": [
        "retriever = EventTypeRetriever(\n",
        "    model_name=f\"{CHECKPOINT_DIR}/retrieve_best_model\",\n",
        "    device=DEVICE,\n",
        "    tokenizer=tokenizer,\n",
        "    event_types=event_types,\n",
        "    max_length=MAX_LENGTH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c39c5b46",
      "metadata": {
        "id": "c39c5b46"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ---------------------- Load samples ----------------------\n",
        "train_samples = load_json_or_jsonl(TRAIN_JSON_PATH)\n",
        "val_samples = load_json_or_jsonl(VAL_JSON_PATH)\n",
        "test_samples = load_json_or_jsonl(TEST_JSON_PATH)\n",
        "\n",
        "# ---------------------- Dataset & DataLoader ----------------------\n",
        "train_dataset = EventArgumentDataset(\n",
        "    samples=train_samples,\n",
        "    ontology_path=ONTOLOGY_PATH,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_LENGTH,\n",
        "    output_max_length=OUTPUT_MAX_LENGTH,\n",
        "    topk_event_types=TOP_K,\n",
        "    retriever=retriever\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "val_dataset = EventArgumentDataset(\n",
        "    samples=val_samples,\n",
        "    ontology_path=ONTOLOGY_PATH,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_LENGTH,\n",
        "    output_max_length=OUTPUT_MAX_LENGTH,\n",
        "    topk_event_types=TOP_K,\n",
        "    retriever=retriever\n",
        ")\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "test_dataset = EventArgumentDataset(\n",
        "    samples=test_samples,\n",
        "    ontology_path=ONTOLOGY_PATH,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_LENGTH,\n",
        "    output_max_length=OUTPUT_MAX_LENGTH,\n",
        "    topk_event_types=TOP_K,\n",
        "    retriever=retriever\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d23e9245",
      "metadata": {
        "id": "d23e9245"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---------------------- Hàm evaluate ----------------------\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=OUTPUT_MAX_LENGTH)\n",
        "            predictions = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
        "            targets = [tokenizer.decode(t, skip_special_tokens=True) for t in labels]\n",
        "\n",
        "            all_predictions.extend(predictions)\n",
        "            all_targets.extend(targets)\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    precision, recall, f1 = compute_metrics(all_predictions, all_targets)\n",
        "    return avg_loss, all_predictions, all_targets, precision, recall, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7d7896a",
      "metadata": {
        "id": "f7d7896a"
      },
      "outputs": [],
      "source": [
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "best_epoch = 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35611c0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35611c0f",
        "outputId": "6f31217c-0562-409e-c5d9-716d6d73c59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best model from epoch 4\n"
          ]
        }
      ],
      "source": [
        "# ---------------------- Load best checkpoint để đánh giá test ----------------------\n",
        "with open(os.path.join(CHECKPOINT_DIR, \"best_checkpoint.txt\"), \"r\") as f:\n",
        "    best_epoch = int(f.read().strip())\n",
        "best_ckpt_path = os.path.join(CHECKPOINT_DIR, f\"bart_best_model_epoch{best_epoch}.pt\")\n",
        "\n",
        "model.load_state_dict(torch.load(best_ckpt_path))\n",
        "model.to(DEVICE)\n",
        "print(f\"Loaded best model from epoch {best_epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6868e20",
      "metadata": {
        "id": "b6868e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f03147c-9bc2-4a54-a868-8576b2c9e279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 97/97 [01:08<00:00,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.0156 | Precision: 0.3282 | Recall: 0.3457 | F1: 0.3263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate\n",
        "val_loss, val_preds, val_targets, p, r, f1 = evaluate(model, val_loader, DEVICE)\n",
        "print(f\"Val Loss: {val_loss:.4f} | Precision: {p:.4f} | Recall: {r:.4f} | F1: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_preds, test_targets, p, r, f1 = evaluate(model, test_loader, DEVICE)\n",
        "print(f\"Test Loss: {test_loss:.4f} | Precision: {p:.4f} | Recall: {r:.4f} | F1: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o6S_xBd3oYS",
        "outputId": "481b09b6-9e02-49f1-c5f3-68c522f89b8d"
      },
      "id": "5o6S_xBd3oYS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 108/108 [01:10<00:00,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0165 | Precision: 0.3104 | Recall: 0.3236 | F1: 0.3060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
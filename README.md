# ğŸ¯ Event Extraction System

Há»‡ thá»‘ng AI tá»± Ä‘á»™ng trÃ­ch xuáº¥t sá»± kiá»‡n vÃ  vai trÃ² tá»« vÄƒn báº£n tiáº¿ng Anh, sá»­ dá»¥ng kiáº¿n trÃºc 2 giai Ä‘oáº¡n vá»›i RoBERTa (Event Detection) vÃ  BART (Role Extraction).

---

## âœ¨ TÃ­nh nÄƒng chÃ­nh

- ğŸ¤– **Event Detection**: Nháº­n diá»‡n 49+ loáº¡i sá»± kiá»‡n (cÃ³ thá»ƒ má»Ÿ rá»™ng vÃ´ háº¡n vá»›i LLM)
- ğŸ“ **Role Extraction**: TrÃ­ch xuáº¥t 5-10 vai trÃ² cho má»—i sá»± kiá»‡n (who, what, where, when, etc.)
- ğŸš€ **Zero-shot Learning**: Há»— trá»£ domain má»›i khÃ´ng cáº§n retrain (LLM fallback)
- âš¡ **Batch Processing**: Xá»­ lÃ½ nhanh vá»›i batch inference
- ğŸ¨ **Beautiful UI**: Giao diá»‡n hiá»‡n Ä‘áº¡i vá»›i visualization vÃ  highlighting
- ğŸ”Œ **RESTful API**: Dá»… dÃ ng tÃ­ch há»£p vÃ o há»‡ thá»‘ng hiá»‡n cÃ³
- ğŸ³ **Docker Ready**: Containerized deployment

---

## ğŸ“‹ YÃªu cáº§u há»‡ thá»‘ng

- **Python**: 3.8+
- **Node.js**: 16+ (cho frontend)
- **RAM**: 8GB minimum (16GB recommended)
- **CPU**: 4 cores minimum
- **Disk**: 10GB cho models vÃ  dependencies
- **GPU**: Optional (tÄƒng tá»‘c 5-10x)

---

## ğŸš€ Quick Start

### Option 1: Docker (Khuyáº¿n nghá»‹)

```bash
# Build vÃ  run cáº£ backend + frontend
docker-compose up --build

# Truy cáº­p:
# - Frontend: http://localhost:5173
# - Backend API: http://localhost:8000
# - API Docs: http://localhost:8000/docs
```

### Option 2: Manual Setup (Development)

#### 1. Backend Setup

```bash
# Clone repo (náº¿u chÆ°a cÃ³)
git clone <repo-url>
cd events_extraction

# Táº¡o virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c: venv\Scripts\activate  # Windows

# CÃ i dependencies
pip install -r requirements.txt

# Download NLTK data (náº¿u cáº§n)
python -c "import nltk; nltk.download('punkt')"

# Cháº¡y backend
python -m uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

#### 2. Frontend Setup

```bash
# Má»Ÿ terminal má»›i
cd event-extract-ui

# CÃ i dependencies
npm install

# Cháº¡y frontend
npm run dev

# Frontend sáº½ cháº¡y táº¡i: http://localhost:5173
```

#### 3. Verify

```bash
# Check backend health
curl http://localhost:8000/health

# Test API
curl -X POST http://localhost:8000/extract \
  -H "Content-Type: application/json" \
  -d '{"text": "A bomb exploded in Baghdad, killing 15 people.", "top_k": 2}'
```

---

## ğŸ“ Cáº¥u trÃºc project

```
events_extraction/
â”œâ”€â”€ app.py                 # FastAPI backend chÃ­nh
â”œâ”€â”€ config.py              # File cáº¥u hÃ¬nh táº­p trung
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ docker-compose.yml     # Docker setup
â”œâ”€â”€ Dockerfile            # Backend Docker image
â”‚
â”œâ”€â”€ checkpoints/          # Model checkpoints
â”‚   â”œâ”€â”€ retrieve_best_model/      # RoBERTa retriever
â”‚   â””â”€â”€ bart_best_model_*.pt      # BART model
â”‚
â”œâ”€â”€ ontoloy/             # Ontology (event types + questions)
â”‚   â””â”€â”€ event_role_WIKI_q.json
â”‚
â”œâ”€â”€ src/                 # Source code
â”‚   â”œâ”€â”€ eventtype_retriever.py    # Event detection
â”‚   â”œâ”€â”€ llm_event_detector.py     # LLM fallback (optional)
â”‚   â””â”€â”€ utils/                     # Utilities
â”‚
â”œâ”€â”€ processing_data/     # Training data
â”‚   â”œâ”€â”€ train.json
â”‚   â”œâ”€â”€ dev.json
â”‚   â””â”€â”€ event_types.json
â”‚
â”œâ”€â”€ event-extract-ui/    # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”‚
â””â”€â”€ scripts/            # Training/evaluation scripts
```

---

## âš™ï¸ Cáº¥u hÃ¬nh

### CÃ¡ch 1: Sá»­a trá»±c tiáº¿p `config.py` (ÄÆ¡n giáº£n)

```python
# config.py
USE_LLM_FALLBACK = True  # Báº­t LLM fallback
OPENAI_API_KEY = "sk-proj-YOUR-KEY-HERE"
LLM_CONFIDENCE_THRESHOLD = 0.5
MAX_LENGTH = 256
TOP_K = 1
```

### CÃ¡ch 2: Environment Variables (Production)

```bash
# Báº­t LLM fallback
export USE_LLM_FALLBACK=true
export OPENAI_API_KEY=sk-proj-YOUR-KEY-HERE
export LLM_CONFIDENCE_THRESHOLD=0.5

# Cháº¡y backend
python -m uvicorn app:app --reload
```

### CÃ¡c tham sá»‘ quan trá»ng:

| Tham sá»‘ | Máº·c Ä‘á»‹nh | MÃ´ táº£ |
|---------|----------|-------|
| `USE_LLM_FALLBACK` | `false` | Báº­t/táº¯t LLM fallback cho event types má»›i |
| `OPENAI_API_KEY` | `None` | API key OpenAI (náº¿u dÃ¹ng LLM) |
| `LLM_CONFIDENCE_THRESHOLD` | `0.5` | NgÆ°á»¡ng confidence Ä‘á»ƒ gá»i LLM |
| `MAX_LENGTH` | `256` | Äá»™ dÃ i input tá»‘i Ä‘a |
| `TOP_K` | `1` | Sá»‘ event types retrieve má»—i cÃ¢u |
| `DEVICE` | auto | Device cho inference (`cpu`, `cuda`, `mps`) |

**Chi tiáº¿t Ä‘áº§y Ä‘á»§:** Xem file `config.py`

---

## ğŸ”Œ API Endpoints

### 1. Extract Events (Detailed)

```bash
POST /extract
Content-Type: application/json

{
  "text": "A bomb exploded in Baghdad, killing 15 people.",
  "top_k": 2
}
```

**Response:**
```json
{
  "events": [
    {
      "sentence": "A bomb exploded in Baghdad, killing 15 people.",
      "event_type": "Conflict.Attack.DetonateExplode",
      "arguments": [
        {"role": "Attacker", "answer": "bomber"},
        {"role": "Place", "answer": "Baghdad"},
        {"role": "Victim", "answer": "15 people"}
      ]
    }
  ]
}
```

### 2. Extract Summary (Aggregated)

```bash
POST /extract-summary

{
  "text": "Multiple sentences text...",
  "top_k": 2
}
```

**Response:**
```json
{
  "top_events": [
    {
      "event_type": "Conflict.Attack.DetonateExplode",
      "frequency": 2,
      "sentences": ["...", "..."],
      "total_roles": 5,
      "roles": [
        {"role": "Attacker", "answer": "bomber", "sentence": "..."},
        {"role": "Victim", "answer": "15 people", "sentence": "..."}
      ]
    }
  ],
  "total_sentences": 3,
  "total_events": 2
}
```

### 3. Health Check

```bash
GET /health
```

**API Docs:** http://localhost:8000/docs

---

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input Text        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: RoBERTa   â”‚  â† Event Type Detection
â”‚  Retriever Model    â”‚     (49 types trained)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€ High confidence (>0.5) â”€â”€â”€â”€â”€â”€â”
           â”‚                                â”‚
           â””â”€ Low confidence (<0.5) â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                        â”‚                   â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
              â”‚  LLM Fallback    â”‚          â”‚
              â”‚  (GPT-4o-mini)   â”‚          â”‚
              â”‚  Zero-shot       â”‚          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                        â”‚                   â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: BART      â”‚  â† Role Extraction
â”‚  Question Answering â”‚     (5-10 roles per event)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output: Events +   â”‚
â”‚  Roles + Answers    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stage 1: Event Type Detection
- **Model**: RoBERTa-base (fine-tuned)
- **Method**: Cosine similarity vá»›i event embeddings
- **Output**: Top-k event types vá»›i confidence scores
- **Fallback**: LLM náº¿u confidence < threshold

### Stage 2: Role Extraction
- **Model**: BART-base (fine-tuned)
- **Method**: Question-Answering (QA) approach
- **Input**: Sentence + Event Type + Questions tá»« ontology
- **Output**: Answers cho má»—i role (who, what, where, when, etc.)

---

## ğŸ§ª Testing

### Test vá»›i curl:

```bash
# Simple test
curl -X POST http://localhost:8000/extract-summary \
  -H "Content-Type: application/json" \
  -d '{
    "text": "A suicide bomber killed 15 people in Baghdad yesterday.",
    "top_k": 2
  }' | python -m json.tool
```

### Test vá»›i Python:

```python
import requests

response = requests.post(
    "http://localhost:8000/extract-summary",
    json={
        "text": "A suicide bomber killed 15 people in Baghdad yesterday.",
        "top_k": 2
    }
)
print(response.json())
```

### Test cases máº«u:

Xem file `DEMO_GUIDE.md` Ä‘á»ƒ cÃ³ danh sÃ¡ch test cases Ä‘áº§y Ä‘á»§.

---

## ğŸ¨ Frontend Features

- **Tab "Chi tiáº¿t"**: 
  - Hiá»ƒn thá»‹ tá»«ng cÃ¢u vÃ  events Ä‘Æ°á»£c phÃ¡t hiá»‡n
  - Roles Ä‘Æ°á»£c highlight trong text gá»‘c
  - Color coding theo event types

- **Tab "Tá»•ng há»£p"**:
  - Statistics tá»•ng quan (sá»‘ cÃ¢u, sá»‘ events)
  - Top 3 events phá»• biáº¿n nháº¥t
  - Danh sÃ¡ch táº¥t cáº£ roles detected

- **Tab "Biá»ƒu Ä‘á»“"**:
  - Pie chart: PhÃ¢n bá»‘ event types
  - Bar chart: Sá»‘ lÆ°á»£ng roles per event
  - Heatmap: Má»‘i quan há»‡ Event-Role

---

## ğŸ”§ Troubleshooting

### Backend khÃ´ng start Ä‘Æ°á»£c

```bash
# Check port 8000 cÃ³ bá»‹ chiáº¿m khÃ´ng
lsof -i :8000
# Náº¿u cÃ³, kill process:
kill -9 <PID>

# Check Python version
python --version  # Cáº§n >= 3.8

# Reinstall dependencies
pip install --upgrade -r requirements.txt
```

### Frontend khÃ´ng load

```bash
# Check port 5173
lsof -i :5173

# Clear npm cache
npm cache clean --force
rm -rf node_modules package-lock.json
npm install

# Check backend connection
curl http://localhost:8000/health
```

### Model khÃ´ng tÃ¬m tháº¥y

```bash
# Check checkpoints folder
ls -la checkpoints/
ls -la checkpoints/retrieve_best_model/

# Náº¿u thiáº¿u, cáº§n download hoáº·c train láº¡i
```

### API response cháº­m

- **CPU inference**: ~2 seconds per document (normal)
- **Cáº£i thiá»‡n**:
  - DÃ¹ng GPU: 5-10x faster
  - Giáº£m `top_k` xuá»‘ng 1
  - Giáº£m `MAX_LENGTH` xuá»‘ng 128
  - Deploy trÃªn server máº¡nh hÆ¡n

---

## ğŸ“Š Performance

| Metric | CPU | GPU (CUDA) |
|--------|-----|------------|
| Inference time per document | ~2s | ~0.3s |
| Throughput | 30 docs/min | 200 docs/min |
| Memory usage | 4GB | 6GB |
| Recommended setup | 8GB RAM, 4 cores | 16GB RAM, NVIDIA GPU |

---

## ğŸ” Báº£o máº­t

- **Self-hosted**: Dá»¯ liá»‡u khÃ´ng rá»i khá»i server
- **LLM optional**: CÃ³ thá»ƒ disable hoÃ n toÃ n (set `USE_LLM_FALLBACK=false`)
- **No logging**: KhÃ´ng log input text (configurable)
- **Docker isolated**: Cháº¡y trong container riÃªng biá»‡t
- **API auth**: CÃ³ thá»ƒ thÃªm token authentication

---

## ğŸ’° Chi phÃ­

### Infrastructure:
- **Self-hosted**: FREE (chá»‰ phÃ­ server)
- **Cloud (AWS t3.xlarge)**: ~$150/month
- **Cloud vá»›i GPU (g4dn.xlarge)**: ~$500/month

### LLM API (Optional):
- **GPT-4o-mini**: $0.0002 per call
- **Typical usage**: 100-1000 calls/month = $0.02-$0.20/month
- **Very affordable!**

---

## ğŸ“š Documentation

- **HÆ°á»›ng dáº«n demo**: Xem `DEMO_GUIDE.md`
- **API Documentation**: http://localhost:8000/docs (khi backend Ä‘ang cháº¡y)
- **Configuration**: Xem `config.py`
- **Training scripts**: Xem thÆ° má»¥c `scripts/`

---

## ğŸ¤ Support & Contact

Náº¿u gáº·p váº¥n Ä‘á»:
1. Check logs trong terminal
2. Check API health: `curl http://localhost:8000/health`
3. Xem pháº§n Troubleshooting á»Ÿ trÃªn
4. LiÃªn há»‡ team

---

## ğŸ“ License

[Add your license here]

---

## ğŸ‰ Quick Start Checklist

- [ ] Clone repo
- [ ] Install Python dependencies: `pip install -r requirements.txt`
- [ ] Install Node dependencies: `cd event-extract-ui && npm install`
- [ ] Start backend: `python -m uvicorn app:app --reload`
- [ ] Start frontend: `cd event-extract-ui && npm run dev`
- [ ] Open browser: http://localhost:5173
- [ ] Test vá»›i sample text
- [ ] Read `DEMO_GUIDE.md` Ä‘á»ƒ biáº¿t cÃ¡ch demo

**Done! Há»‡ thá»‘ng sáºµn sÃ ng sá»­ dá»¥ng! ğŸš€**
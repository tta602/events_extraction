{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e3d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from src.event_argument_dataset import EventArgumentDataset\n",
    "from src.eventtype_retriever import EventTypeRetriever\n",
    "from src.utils.data_utils import build_labels, load_json_or_jsonl\n",
    "from src.utils.device_util import getDeviceInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16421e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Config ----------------------\n",
    "DEVICE = getDeviceInfo()\n",
    "BART_MODEL = \"facebook/bart-base\"\n",
    "MAX_LENGTH = 128\n",
    "OUTPUT_MAX_LENGTH = 64\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "LR = 3e-5\n",
    "TOP_K = 3\n",
    "CONTEXT_PATH = \"\"\n",
    "CHECKPOINT_DIR = f\"{CONTEXT_PATH}checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_JSON_PATH = f\"{CONTEXT_PATH}processing_data/train.json\"\n",
    "VAL_JSON_PATH = f\"{CONTEXT_PATH}processing_data/dev.json\"\n",
    "TEST_JSON_PATH = f\"{CONTEXT_PATH}processing_data/test.json\"\n",
    "ONTOLOGY_PATH = f\"{CONTEXT_PATH}ontoloy/event_role_WIKI_q.json\"\n",
    "LABEL_CACHE_PATH = f\"{CONTEXT_PATH}processing_data/event_types.json\"\n",
    "\n",
    "event_types = build_labels(TRAIN_JSON_PATH, LABEL_CACHE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08653b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Load tokenizer & model ----------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(BART_MODEL)\n",
    "special_tokens = [\"<tgr>\"]\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(BART_MODEL).to(DEVICE)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7479f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = EventTypeRetriever(\n",
    "    model_name=f\"{CHECKPOINT_DIR}/retrieve_best_model\",\n",
    "    device=DEVICE,\n",
    "    tokenizer=tokenizer,\n",
    "    event_types=event_types\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbaada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------- Load samples ----------------------\n",
    "train_samples = load_json_or_jsonl(TRAIN_JSON_PATH)\n",
    "val_samples = load_json_or_jsonl(VAL_JSON_PATH)\n",
    "test_samples = load_json_or_jsonl(TEST_JSON_PATH)\n",
    "\n",
    "# ---------------------- Dataset & DataLoader ----------------------\n",
    "train_dataset = EventArgumentDataset(\n",
    "    samples=train_samples,\n",
    "    ontology_path=ONTOLOGY_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    output_max_length=OUTPUT_MAX_LENGTH,\n",
    "    topk_event_types=TOP_K,\n",
    "    retriever=retriever\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = EventArgumentDataset(\n",
    "    samples=val_samples,\n",
    "    ontology_path=ONTOLOGY_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    output_max_length=OUTPUT_MAX_LENGTH,\n",
    "    topk_event_types=TOP_K,\n",
    "    retriever=retriever\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "test_dataset = EventArgumentDataset(\n",
    "    samples=test_samples,\n",
    "    ontology_path=ONTOLOGY_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    output_max_length=OUTPUT_MAX_LENGTH,\n",
    "    topk_event_types=TOP_K,\n",
    "    retriever=retriever\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f880b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------- Hàm evaluate ----------------------\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=OUTPUT_MAX_LENGTH)\n",
    "            predictions = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
    "            targets = [tokenizer.decode(t, skip_special_tokens=True) for t in labels]\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_targets.extend(targets)\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, all_predictions, all_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f96d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------- Training loop + checkpoint ----------------------\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "best_val_loss = float(\"inf\")\n",
    "best_epoch = -1\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"BART Epoch {epoch}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch} finished. Avg train loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_preds, val_targets = evaluate(model, val_loader, DEVICE)\n",
    "    print(f\"Validation loss after epoch {epoch}: {val_loss:.4f}\")\n",
    "\n",
    "    # Lưu checkpoint tốt nhất\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        ckpt_path = os.path.join(CHECKPOINT_DIR, f\"bart_best_model_epoch{epoch}.pt\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        # Lưu thông tin best epoch\n",
    "        with open(os.path.join(CHECKPOINT_DIR, \"best_checkpoint.txt\"), \"w\") as f:\n",
    "            f.write(str(best_epoch))\n",
    "        print(f\"Saved best model checkpoint to {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Load best checkpoint để đánh giá test ----------------------\n",
    "with open(os.path.join(CHECKPOINT_DIR, \"best_checkpoint.txt\"), \"r\") as f:\n",
    "    best_epoch = int(f.read().strip())\n",
    "best_ckpt_path = os.path.join(CHECKPOINT_DIR, f\"bart_best_model_epoch{best_epoch}.pt\")\n",
    "\n",
    "model.load_state_dict(torch.load(best_ckpt_path))\n",
    "model.to(DEVICE)\n",
    "print(f\"Loaded best model from epoch {best_epoch}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_preds, test_targets = evaluate(model, test_loader, DEVICE)\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
